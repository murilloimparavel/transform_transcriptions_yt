What's up everybody? If you're like me, you get a little bit FOMOed and overwhelmed with the seemingly thousands 
of MCP servers that have come out. Which ones are good? Which ones are for coding 
specifically? Well, at this point, I've tried out a few dozen different MCP servers. Some of them are good. Some of 
them were so so. Some of them were super concrete for specific situations. Some 
of them were too abstract that I could even get any value out of them. So, I said, "Hey, wouldn't the internet 
appreciate if I went through all of them and broke down the ones that I actually still use every single week?" If you 
don't know me, my name is Sean. I've worked in different SAS companies before, and I also helped build and 
scale a marketing company to over 8 figures in annual revenue. And so, from 
all of that experience, I like to bring to YouTube real life practical tutorials 
for things that actually work in the real world. So, that's what we're going to do. My top nine MCP servers that I 
use every single week. We're going to go through each of them. We're going to talk about what they are, where I actually use them in my process, and 
what it means for your overall development workflows, aka should you actually consider using them or not. So, 
if you like doing more with less, this video is for you. And spoiler alert, one of these MCPs is from a smaller 
developer, but I still love it. So, stick around till the end for that one. or shamelessly check out the timestamps 
below and just fast forward to it, but then YouTube might not show my content to people like you and I'll be doomed to 
20,000 subscribers for the rest of eternity. So, let's get into it. So, first up on the app is the linear MCP. 
So, Linear is an app that honestly allows you to build a lot faster and a lot more systematically. And it does 
that by allowing you to track issues that you're having, but also manage larger feature builds and add things 
into the backlog as you come across them in your development process. Because if you're like me, things come up all the 
time when you're building and just jotting those down like a notepad is a terrible way to do things. And so it's 
really nice to have a tool like Linear integrated as an MCP into your 
development workflows. So let's take a look at an example. So, a lot of people get lost in the sauce when they're 
building things. So, in this example, I'm asking Claude to analyze my app for performance bottlenecks and recommend 
some different optimizations. And so, what inevitably happens is that when this thing completes, and again, this 
could be a bug. It could be a feature that you've thought of, it could be something you know you need to add to your backlog. It could be something 
that's a critical blocker that you just don't want to fix right now. And so you need to sync the work that's happening 
in the app right now with a backlog that you can check and prioritize and work 
on. And so this is really important because your backlogs need to be prioritized. You need to know what's 
important to fix, what's important to build on. How are you going to decide if you're in building mode or fixing mode 
or whatever mode? Well, linear is great because when these things crop up in the course of your development and you're 
making those decisions, you can add them into your backlog. So let's take a look 
at what that looks like. And so it's found some issues already like how I'm querying my database, issues with how 
I'm caching information on the front end and a few different things. And so what 
we can do is we can use the linear MCP to actually prioritize this backlog. So 
let's take a look at what that looks like. So I can tell it use the linear MCP to prioritize these in order of 
impact and give implementation notes. And now we can see as this process is 
nearing completion. And so it went through and it created a bunch of tickets basically for me to um address. 
And so we can see all of these from October 3rd. We have different bugs and then we have different rooms for improvement too. And so if we wanted to 
look at for example the cache invalidation strategy, we can come in, 
we can look at it, we can see what the impact is that it's having notes about the implementation with specific code 
snippets and then we have different solutions that we can use to implement this. Now, why is this important? Well, 
later on, call it 2 days from now, when we go in and we want to look at this specific issue. So, for example, this 
one's 81. If we went back into our project and had no context for that conversation that just took place, we 
could say pull from linear MCP this issue and summarize it. And now we could 
say or build an implementation plan, right? And so on. And so what it's going to do is it's going to go and it is 
going to fetch that issue from linear and then it'll pull it in and from there we could work on it. There we go. We 
have an exact definition essentially of the problem. And so this is great because AI coding moves very fast but if 
you don't have systems in place that velocity creates chaos in your process 
and then you lose track of important things to fix as well as important things to maybe build later on. So if 
you want to develop with AI sustainably, you need to be able to capture these things and surface them so that you can 
continue moving forward and working on the important things that need to be worked on. So the next MCP that we're 
going to look at is Perplexity's MCP. And there's a lot of different reasons why this becomes very valuable. It's 
helpful when you're researching for new features, especially in an area that you're not an expert in necessarily. Say 
you want to go out and start building agents, but you're not, you know, incredibly talented at building agents 
and you want to understand what are the patterns, what are the best practices. Perplexity is something that's going to 
help you do that. It's also really helpful when you're trying to debug certain problems or understand the 
solution that you should try to take to a specific problem. Perplexity can go out and it can find other people that 
have hit that type of problem before. And so without awareness about the types of things that we're building and what 
the best practices are, you're never going to build something that's truly optimized. So perplexity helps us in 
that research phase. So one way that we could use this is literally right here where we just left off. So we have these 
proposed solutions for how we can solve that caching issue. Well, we could ask Perplexi to go out and find best 
practices for doing that thing. But let's say that we want to do research for a new feature that we're going to 
build. So here's an example. I want to build a new feature that utilizes the Gemini API to understand what exactly is 
inside of an image that a user uploads. For example, like the objects in the image and allow a user to tap on them 
and erase them from the image. Use the Perplexity MCP to determine the best approach. And within a few seconds, 
realistically, I think that was like no more than 60 seconds, we now have a concrete recommendation for exactly how 
to do that. So we can use the Gemini flash API for segmenting the images and 
it can in fact identify multiple objects in a single API call which is pretty cool. Then it's saying basically just 
display those boxes in a interactive UI flow where the users can click on it and 
then you can use an inpainting model to actually provide that capacity to be able to remove the things from the 
images and so perplexity is really undervalued for what it is. It's not just going to go find information. and 
it's going to understand the context of what you really want and combine multiple searches together to give you 
an actual plan. So, I love using this process for trying to understand the best pattern or workflow or APIs or 
implementation plan for technology or frameworks that I really don't know much about. And so, to zoom out for a second, 
AI can code faster than you think, but it needs to know what you consider good 
and what direction you want to take with what you are building. And so that gap between what is possible and what is 
optimal, a tool like perplexity helps us bridge that gap and understand what we 
can feasibly do and then integrate that into our development process. So modern 
vibe coding isn't about guessing or just giving the decision-m to the language 
model. It's about giving it the right context to make informed decisions. And 
that is what perplexity helps us do. So the next MCP server is GitHub's MCP. So 
proper repository management, issue management, branching, all of that type 
of stuff. It can actually be quite involved for someone that is more on the beginner side. So AI coders that are new 
to development can struggle with some of those fundamentals that might be second nature to someone that's been doing it 
for a decade. And trust me, I see it in my comments. This guy used he used a 
flux capacitor to connect to his widgety woo while he was trying to traverse his git trees in real time. What a noob. But 
the fundamental issue here is that working in only one branch is going to create such big issues for you that if 
you don't get in the habit of proper version control systems, you are going to be screwed eventually. And so the use 
cases that I fall into most for GitHub's MCP is the general repository 
management. So looking at commits that have been made across the project and trying to understand certain things and 
then creating issues and automating the creation of pull requests so that we can 
track mergers that we're trying to make back into our main project and and all of that type of stuff. Which again, even 
if you're just vibe coding a project, you want to get in the habit of thinking, hey, one day this is really going to be something and I I better get 
into the development best practices now and not just go balls to the wall 
working in one branch for the rest of my life. So, here's an example. We're going to create a new bug branch to tackle 
this goal 78 issue that we have. And then I want you to use the GitHub MCP to 
create an issue for it. Once you've done that, fix the bug. Once you've done that, use the GitHub MCP to create a 
pull request that fixes the issue, leave feedback on the changes, and mark the issue complete. So, if we were to now 
pop back over into GitHub and we went to our repository, we can see that there's now an open poll request. And so, if we 
were to go in here and look at it, we can see a summary of what the problem actually was and what the solution was 
to fix it. Now I have a separate linting issue which is why this is not deploying to my project on Verscell. But we could 
come through here. We could run tests. We could review the actual file changes 
that were made. And if all is good, we can actually merge this into our main 
branch. So if we were to now pop back into our main branch, boom, we can see that we just pushed this pull request 
through. So speed without structure is just technical debt waiting to happen. and it's just in disguise. Hasn't shown 
its face yet, but it's there. And so, while AI is democratizing the coding process, it's still important that 
you're thoughtful and deliberate about what you are doing and that you have systems behind what you are doing. So, 
the best AI developers are not skipping fundamentals. They are just automating as much of it as makes sense. So, the 
next MCP we're going to look at is the Superbase MCP. Now, the reason we're doing this, this is a very valuable 
development tool. If you are using Superbase, which I know a lot of you do. Now, the reason it's valuable is that 
when you're developing, especially when you're running into issues in your code, you inevitably need to look at the 
different tables that you have and the entries in order to determine why something isn't happening the way that 
you think it should be happening, right? And so, you need to be able to actually reference the state of your database to 
see what is broken and what's not. Now, if you were not using this MCP server, your options would be to make a manual 
SQL query and have to actually run that thing from inside of your coding environment. Or number two, actually 
going in like I'm showing here and looking at our database tables and 
trying to understand, hey, what columns are there, what columns are supposed to be there that aren't, which ones are that shouldn't be, and you know, all of 
that type of stuff. And trying to determine what exactly is the problem. knowing what columns to actually pull 
and send back into the system to give it example data. Very annoying headache to have to deal with. So why do that when 
we could just use an MCP? So for this one, let's take a look at a an app that we're working in. So we've been working 
on this in a few different videos on the channel. It's like a generalized prompt uh repository type of app. I'm not going 
to get into that now, but it basically has this functionality where we can come in, we can store prompts, then we can edit those prompts and keep a live 
version history of the edits that we made. Now, let's say that we had made a 
bunch of edits to this prompt specifically, but none of the versions were actually showing. So, what we could 
do is say, hm, I mean, I know I made those changes, so it should be there. Let's try to figure out what's happening 
because it might be the case that the tables weren't created properly. Could be that the entries aren't being injected in there. Could be that the 
like the ID keys aren't matching up, so it doesn't know to pull in for this 
specific prompt, these specific versions. There's a lot of things that could be the case. And so we could say 
version history isn't loading for one of my prompts called design maker, which is the one we were just looking at. Use the Superbase MCP to help me understand why. 
And so we can see what this looks like inside of our project where they use the Superbase MCP to investigate everything. 
And then instead of us having to actually create, you know, a SQL query to go find all this stuff, it is going to go out and it's going to find it. So 
it first finds the actual project that we're talking about, then it goes and it finds the title of the prompt we were 
talking about, right? And so it understands our project and what exactly we're saying and it knows where to go 
find that thing. And so the result of this one obviously was that it did in fact find them because we do know they exist. So obviously in this case that 
was just a user error. But this can be incredibly helpful when you start running into issues where it's clear 
that there's a disconnect between your app and the database and how information 
is moving between the two. So vibe coding really works well when you can easily match your expectations to the 
reality of what's happening and making sure that there isn't a disconnect between those two worlds which a tool 
like this helps us do pretty easily. So next up on the list is a tool called context 7. So the fundamental issue with 
a lot of language models is that they still make stuff up sometimes. And so when you are building something new AI 
often uses outofdate documentation. So you'll see this where they hallucinate endpoints that don't exist. They like 
make up functions that don't exist and you end up, you know, building a lot of architecture around something that isn't 
actually the way that it works. And so the thing we need to solve for is having always upto-date documentation for 
whatever framework or library it is that you are using. And the tool that I like to do that with is called context 7. So 
here's like a very direct like linear kind of way to use something like this. 
So I want to build a multi- aent team that can improve prompts for users which again is part of this kind of app that 
we're building. Let's say it does this or the way that I want it to happen is that it uses a root orchestrator with 
access to various tools and personas like a researcher that's maybe powered by perplexity a subject matter expert if 
this for example is for like a coding domain. um a writer that can actually rewrite the prompt. So, it has access to 
all of these different agents, but I need to really understand best practices and conventions for crew AI in order to 
do this. And I want to use context 7 to pull that documentation and help me build that understanding. And so, what 
it does is it first goes out, it searches for the name and gets the library ID for that name. And then it's 
going to go through and it's going to start searching through that library to pull me the relevant documentation. that 
gets the answer really that I am looking for. Now the only potential downside 
with context 7 is it does return a lot of tokens but there are paid options 
that will help you cut this down like ref for example which is an alternative to context 7 that does not return you 
the entire list of documentation kind of intelligently determines what it's going to deliver you and so this pattern of ro 
goal backstory this is how crewi works so it was able to pull this out from its documentation and then start giving 
examples it's showing us how to actually design the tasks. It's showing us the type of processes that they actually 
have. So when you're building agents in a tool like crew, you have this concept of sequential tasks that go one after 
another or hierarchical tasks where this root orchestrator can delegate to 
specialists kind of intelligently. And so it's pulling in really a lot of really great context based on the 
question that we asked. So, context 7 is really great for research about how to 
best implement this thing as well as pulling documentation for something that you already know you're doing. Like if I 
wanted to like optimize a superbase query, I could pull in the superbase documentation and use that to improve uh 
what it is that I'm doing. So AI's greatest strengths in that it's always confident about what it's doing is 
simultaneously one of its biggest weaknesses in that it will make up responses to things and convince you 
that it is true. So the future of like how to use AI properly in coding or in business or in any domain of life is 
knowing how exactly to give the system the best context that it needs so that 
it can still move super quick but within a specific lane of context that you provide. And this is why experts will 
never go away because they provide that foundational knowledge and they push the frontiers of things. So the next MCP 
that I I use regularly is the Playright MCP. So Playright is typically used for 
automating like endtoend browser testing, meaning it can actually interact with the browser and test 
things that are going on there. The use case that I love to use Playright for is 
in creating what I call selfgrading UIs. Meaning we use the model to 
automatically fix itself. And what I mean by that is that the model can automatically grade its own work against 
the standards that we have documented. And so what it does is it creates this iterative improvement through a system 
that checks itself. And so here is typically how this process works. So we go through and we build something. Now 
in this case, let's say it's like a web app and we're inside of a browser. Now what typically happens before we get to 
this stage of having it in the browser and testing it and all of that type of stuff, we've built ideally some sort of 
UXUI system that this thing should be building toward. And so the question becomes when it's done, did it actually 
build what we wanted it to build the way that we wanted it to do? And what often 
happens is that it'll take shortcuts and you know you can have this great design system but it doesn't actually implement 
on it properly. And so what this system does is it allows us to use Playright as 
an MCP to take browser snapshots, then grade those snapshots against that 
standard that we had set for how we want our UI made. And then it sends that 
feedback to the LLM so that the LLM can go fix what mistakes it made. And we can 
allow this cycle to continue back and forth, back and forth, back and forth until we have a UI that we are happy 
with. So the reason I love this is that one of the next big evolutions inside of AI, not just in coding, is that we use 
the language model to generate the output, but then based on our objective standards of what we want, we force it 
to reflect on what it did and improve itself over time. So these self-correcting loops help you turn AI 
into something that's a partner that can actually help you build the quality that you are looking for through iteration. 
So the next MCP up on the list is called SEM grip. Now the thing is you don't know what you don't know. That's one of 
the biggest problems that vibe coders face. You can create security 
vulnerabilities that are very obvious to someone that knows really knows what they're doing. You can do that without 
even really realizing it. And so when things aren't being done the way that they should be done, you really don't 
want to deploy something that is fundamentally broken or not secure, especially when it comes to security, 
right? People's personal data and information. And so this is one of the reasons I love Smrep because it allows 
us to automate security checks inside of our code. And so again, we're inside of 
this prompt wallet application and we can simply tell it, I want you to run Smrep to understand my project, its 
vulnerabilities, how bad they are, if there are any, and give me back basically a report. And so 
basically, this does like a minimal scan at first. So it checked my project authentication handlers were fine, 
server actions are fine, middleware, and the configuration of the middleware was good, and the way that we're handling 
our environment variables. So based on the rule set that they have in their 
database, uh it checked out for these eight files. Now what I asked to do was 
run a more comprehensive scan of the entire codebase. So that is what it's off doing now. So basically all of these 
vulnerabilities that it did find after running through that thing, they don't come down to the code themselves. It's 
more about the the dependencies that we have and making sure that they're actually up to date with where they need 
to be. And so it gives me my recommended actions. Updating clerk which is used for the authentication. So very 
important to do updating Nex.js and then updating my ESL lint. Now obviously if 
you're using something like linear MCP which we talked about earlier, you can make this a task for your team or 
yourself to do and then prioritize it based on its importance to you which should be important because it's 
security. So moving fast is really only valuable in so much that what you're building can scale and is secure because 
if you're not doing that, it's kind of like you're just in a car and you close your eyes and slam on the gas pedal. 
You're going to go pretty fast, but eventually you're going to crash and break some So if you don't want security holes, consider using Simgrip. 
So this next MCP is called Vibe Check. And this is the one that I mentioned was from a smaller developer. And what I 
mean by that is it's not a repository that has thousands of stars and hundreds of forks or anything like that, but it 
can still be valuable for you in your vibe coding process. So, like I mentioned, the tool is called Vibe 
Check, and it describes itself as a plug-and-play metacognitive oversight layer for autonomous AI agents, a 
researchbacked MCP server keeping LLMs aligned, reflective, and safe. So, what 
does that mean? Well, if we go down to the problem, there's this thing that I'm sure you've seen before if you've tried 
to build like kind of ambitious projects and you're just letting it go where it just starts really starting to 
overengineer stuff and it's kind of like snowballing into this very complicated thing very fast. And so the author of 
this repository talks about this concept of pattern inertia and reasoning lock in 
which means a large language model can very confidently follow a flawed plan. 
And so if you don't have this external nudging in place, you may get stuff that's really misaligned or way 
overengineered. And so this vibe check MCP provides these like reflective pauses in the 
middle of your process to make sure that we're not taking for granted certain 
assumptions and that we're building in the right direction. And so it's pretty cool because this is a researchbacked um 
approach to things where it's called a chain pattern interrupt which injects brief well-timed pause points at risk 
inflection moments to realign the agent around what the user is actually trying to do and preventing that situation 
where it just snowballs into some crazy thing that like dude I wasn't even trying to build this. That is what we 
are avoiding with this. And it's cool because you can integrate this with anything really that you are building. 
And so there's a few different tools that it has to do this. The first is Vibe check, which the author recommends 
that you actually put this like in your system settings, like in your main agent 
definition, which gets added in as context anytime you're going to use a model. And then if it finds some sort of 
fundamental mistake that the model tends to keep going down, it can capture those mistakes and store them so that it stops 
making them over and over and over again. And so the first thing is that we need to have the MCP installed. And the 
second is that we need to have this kind of mandatory field in this case in our claw.markdown file that tells the system 
that it needs to use this thing. And so specifically, it's saying you need to call vibe check after planning and 
before major actions. So let's see if we can force this thing to trigger so that you can see what it looks like. And so 
we can see as this thing is running what it actually looks like. When this vibe check runs, it's going to look at 
basically what we were intending to do, like what the goal was and what the plan 
was and how well we actually moved toward achieving that. And it seems to 
think we did a great job with uh with this one. And so what it's doing is it's going back one more time and now it's 
rechecking exactly what it did. So it's saying vibe check raises an excellent point about verification. We should also 
add a prefers reduced motion support to the frame or motion component. Let me add that. And so now it's going through 
and it's tightening up its implementation a bit. And now it is learning from the mistake it made. Now 
let's see what happens if instead we just try to do something pretty ambiguous because at the end of the day those were very well- definfined tasks. 
But what if we just said something like, "Hey, go build me a service that does this thing." And make it kind of ambiguous and see if we can get the vibe 
check to come in and give us better feedback. So now we can see what this looks like in real time when we're 
trying to do something that maybe we shouldn't be doing it the way we're intending to. And so again, we gave this 
kind of ambiguous, hey, go build me this crew AI service complete with the research and a writer, right? That's 
like pretty, I mean, ambiguous at the end of the day. And so the vibe check tool is now asking for a check. And so 
we're going to let this thing run and see what type of output we get back. And so we can see what this looks like where 
it says, okay, this like plan is kind of clear to start with, but then there's all these ambiguities. And so this right 
here that we see, this is vibe check basically going back and forth with Claude in this case telling it we need 
to consider these ambiguities and based on your feedback to me, we need to implement certain things and not 
implement other things. And so, especially for people that are are vibe coding and you're trying to push the limits of your own understanding, but 
you also have this thing in the back of your head where you know you want to do things the right way and not go way off 
the rails and do something dumb. This type of MCP is going to give a lot of really nice checkpoints into the system 
so that you know to pause and kind of reflect and that you have this kind of meta layer that's reflecting on exactly 
what you're doing and whether or not you're doing it right, whether you're doing something completely wrong, 
whether you need to pause and say, "Hold on a second. There's some things I need to think about." This is a great tool to 
force that reflection on you. So this helps us turn our coding tool into a hey go do what I say type of relationship to 
hey help me think about if this is the right move and what I might not be considering right now. Now the last tool 
on this list I do really enjoy especially the concept of what it is and 
I think this type of tool is honestly going to become more and more integrated into our lives as the AI industry starts 
maturing. So, I really love this tool and it's actively developed and it's called pieces. So, what this tool does 
is you can configure any number of apps that you have running on your machine that you want it to basically watch, 
meaning it will look at those screens and what's happening and it will use 
that information. And so maybe you're hopping for example from your terminal into cloud desktop into cloud code uh 
into your you know simulator on iOS and you're popping around working on the same fundamental thing across a project 
or a set of tasks. And what this system will do is it will form memories around 
what you actually did and it will allow you to find memory. So, for example, if 
I wanted to look at where I was dealing with a tailwind configuration, I can search through those things. And I can 
also have conversations about what was it specifically inside of this memory 
that we were dealing with. Maybe it was a bug and I want to try to remember what 
type of solution we found. I can use its system to have a conversation about 
again what was happening and what we went through and how we ultimately solved the problem. So just imagine 
there was a separate version of yourself behind you all the time watching everything you were doing and 
remembering what solutions you came to or blockers that you never quite got past. Now to get out in front of it, 
this is all locally processed. What that means is everything that it's doing, all 
the machine learning is actually running on your machine. Now, you can go into 
your settings and you can choose to run those things in a cloud environment if you want to, but I currently have it all 
running on my actual local device. So, again, really cool. Comes with a llama 
on it installed and activated. So I could come through and I could use specific open- source models to run 
through everything on or I can choose, you know, a cloud-based provider and send my requests through there. So 
again, to give you a really concrete example of where this would be useful, I had to debug this authentication error 
like two months ago with a GraphQL project and how amazing it would be if I 
was able to just come back and see what that solution actually was so that I 
could then integrate it into my current project. So now on the topic of MCPs, 
all of this information does get exposed out if you want it to as an MCP. And 
they have a nice little guide here explaining to you how you can actually integrate it into your workflow. So you 
can use it with tools like cursor, github copilot, claw desktop, or if you're really industrious, you could 
choose to actually just run the MCP locally on your computer and hook it into cloud code. So whichever way you 
slice it, it is a super valuable tool. Now, if you are using VS Code like I am 
in this example, you can easily just use it with GitHub's C-pilot and integrate it into all of your systems and 
processes. So, AI coding accelerates nearly everything except learning from 
your mistakes. A lot of us tend to repeat those mistakes over and over and over again because the AI creates the 
problem, the AI solves the problem, and then we just say, "Hey, on to the next one." And so how great is it to have a tool that allows us to look back and 
reflect on those solutions so that we can pull them forward in time. So again, the difference between a beginner and an 
expert is not just the skill, it's the ability to recognize the patterns and 
know how to reuse solutions. So there we go. MCPS that are practically useful 
that I use nearly every single day. So, on this topic, let me know below if you 
want to see how to actually build an MCP server because it's a project that I've 
been thinking about a little bit, but I haven't gotten around to actually building the one that I have in mind, and it could be cool to do alongside 
everyone. So, make sure to like and subscribe and join the group below if you want a dope community of people that 
are building real projects and giving each other feedback and trying to do cool stuff. Who doesn't want to be a 
part of that? That is it for this video. I will see you in the next